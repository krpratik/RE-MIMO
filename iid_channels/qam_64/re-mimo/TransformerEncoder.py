import torch
import torch.nn as nn
import copy

def _get_clones(module, N):
	return nn.ModuleList([copy.deepcopy(module) for i in range(N)])
	
class TransformerEncoder(nn.Module):

	def __init__(self, encoder_layer, num_layers, norm=None):
		super(TransformerEncoder, self).__init__()
		self.layers = _get_clones(encoder_layer, num_layers)
		self.num_layers = num_layers
		self.norm = norm

	def forward(self, src, common_input, index, save_attn_weight, mask=None, src_key_padding_mask=None):
		output = src
		for mod in self.layers:
			output = mod(output, common_input, index, save_attn_weight, src_mask=mask, src_key_padding_mask=src_key_padding_mask)
		if self.norm is not None:
			output = self.norm(output)
		return output